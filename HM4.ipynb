{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "HM4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qf8aBLH9C3HH"
      },
      "source": [
        "# Home 4: Build a CNN for image recognition.\n",
        "\n",
        "### Name: Siddesh Gannu\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocqIk6dyC3HR"
      },
      "source": [
        "## 0. You will do the following:\n",
        "\n",
        "1. Read, complete, and run the code.\n",
        "\n",
        "2. **Make substantial improvements** to maximize the accurcy.\n",
        "    \n",
        "3. Convert the .IPYNB file to .HTML file.\n",
        "\n",
        "    * The HTML file must contain the code and the output after execution.\n",
        "    \n",
        "    * Missing **the output after execution** will not be graded.\n",
        "    \n",
        "4. Upload this .HTML file to your Google Drive, Dropbox, or Github repo. (If you submit the file to Google Drive or Dropbox, you must make the file \"open-access\". The delay caused by \"deny of access\" may result in late penalty.)\n",
        "\n",
        "4. Submit the link to this .HTML file to Canvas.\n",
        "\n",
        "    * Example: https://github.com/wangshusen/CS583-2020S/blob/master/homework/HM4/HM4.html\n",
        "\n",
        "\n",
        "## Requirements:\n",
        "\n",
        "1. You can use whatever CNN architecture, including VGG, Inception, and ResNet. However, you must build the networks layer by layer. You must NOT import the archetectures from ```keras.applications```.\n",
        "\n",
        "2. Make sure ```BatchNormalization``` is between a ```Conv```/```Dense``` layer and an ```activation``` layer.\n",
        "\n",
        "3. If you want to regularize a ```Conv```/```Dense``` layer, you should place a ```Dropout``` layer **before** the ```Conv```/```Dense``` layer.\n",
        "\n",
        "4. An accuracy above 70% is considered reasonable. An accuracy above 80% is considered good. Without data augmentation, achieving 80% accuracy is difficult.\n",
        "\n",
        "\n",
        "## Google Colab\n",
        "\n",
        "- If you do not have GPU, the training of a CNN can be slow. Google Colab is a good option.\n",
        "\n",
        "- Keep in mind that you must download it as an IPYNB file and then use IPython Notebook to convert it to HTML.\n",
        "\n",
        "- Also keep in mind that the IPYNB and HTML files must contain the outputs. (Otherwise, the instructor will not be able to know the correctness and performance.) Do the followings to keep the outputs.\n",
        "\n",
        "- In Colab, go to ```Runtime``` --> ```Change runtime type``` --> Do NOT check ```Omit code cell output when saving this notebook```. In this way, the downloaded IPYNB file contains the outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlOpQjO7C3HS"
      },
      "source": [
        "## 1. Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODTmDCBhC3HS"
      },
      "source": [
        "### 1.1. Load data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FX698hNfC3HT",
        "outputId": "19b072be-d983-4b23-cee4-d15f747f8cc9"
      },
      "source": [
        "from keras.datasets import cifar10\n",
        "import numpy\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "print('shape of x_train: ' + str(x_train.shape))\n",
        "print('shape of y_train: ' + str(y_train.shape))\n",
        "print('shape of x_test: ' + str(x_test.shape))\n",
        "print('shape of y_test: ' + str(y_test.shape))\n",
        "print('number of classes: ' + str(numpy.max(y_train) - numpy.min(y_train) + 1))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape of x_train: (50000, 32, 32, 3)\n",
            "shape of y_train: (50000, 1)\n",
            "shape of x_test: (10000, 32, 32, 3)\n",
            "shape of y_test: (10000, 1)\n",
            "number of classes: 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1b8WqB4C3HT"
      },
      "source": [
        "### 1.2. One-hot encode the labels\n",
        "\n",
        "In the input, a label is a scalar in $\\{0, 1, \\cdots , 9\\}$. One-hot encode transform such a scalar to a $10$-dim vector. E.g., a scalar ```y_train[j]=3``` is transformed to the vector ```y_train_vec[j]=[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]```.\n",
        "\n",
        "1. Define a function ```to_one_hot``` that transforms an $n\\times 1$ array to a $n\\times 10$ matrix.\n",
        "\n",
        "2. Apply the function to ```y_train``` and ```y_test```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvFMcJVpC3HU",
        "outputId": "7c3bdec1-0953-4b43-d182-36ca4b33d89d"
      },
      "source": [
        "def to_one_hot(y, num_class=10):\n",
        "    results = numpy.zeros((len(y), num_class))\n",
        "    for i,y in enumerate(y):\n",
        "      results[i,y] = 1\n",
        "    return results\n",
        "\n",
        "y_train_vec = to_one_hot(y_train)\n",
        "y_test_vec = to_one_hot(y_test)\n",
        "\n",
        "print('Shape of y_train_vec: ' + str(y_train_vec.shape))\n",
        "print('Shape of y_test_vec: ' + str(y_test_vec.shape))\n",
        "\n",
        "print(y_train[0])\n",
        "print(y_train_vec[0])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of y_train_vec: (50000, 10)\n",
            "Shape of y_test_vec: (10000, 10)\n",
            "[6]\n",
            "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNxXtRFcC3HU"
      },
      "source": [
        "#### Remark: the outputs should be\n",
        "* Shape of y_train_vec: (50000, 10)\n",
        "* Shape of y_test_vec: (10000, 10)\n",
        "* [6]\n",
        "* [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFNnL3pKC3HU"
      },
      "source": [
        "### 1.3. Randomly partition the training set to training and validation sets\n",
        "\n",
        "Randomly partition the 50K training samples to 2 sets:\n",
        "* a training set containing 40K samples\n",
        "* a validation set containing 10K samples\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KXKXlhrC3HV",
        "outputId": "7e73915f-78be-4c17-f9ed-1a44411c14a9"
      },
      "source": [
        "rand_indices = numpy.random.permutation(50000)\n",
        "train_indices = rand_indices[0:40000]\n",
        "valid_indices = rand_indices[40000:50000]\n",
        "\n",
        "x_val = x_train[valid_indices, :]\n",
        "y_val = y_train_vec[valid_indices, :]\n",
        "\n",
        "x_tr = x_train[train_indices, :]\n",
        "y_tr = y_train_vec[train_indices, :]\n",
        "\n",
        "print('Shape of x_tr: ' + str(x_tr.shape))\n",
        "print('Shape of y_tr: ' + str(y_tr.shape))\n",
        "print('Shape of x_val: ' + str(x_val.shape))\n",
        "print('Shape of y_val: ' + str(y_val.shape))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of x_tr: (40000, 32, 32, 3)\n",
            "Shape of y_tr: (40000, 10)\n",
            "Shape of x_val: (10000, 32, 32, 3)\n",
            "Shape of y_val: (10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sNoJ7HYC3HV"
      },
      "source": [
        "## 2. Build a CNN and tune its hyper-parameters\n",
        "\n",
        "1. Build a convolutional neural network model\n",
        "2. Use the validation data to tune the hyper-parameters (e.g., network structure, and optimization algorithm)\n",
        "    * Do NOT use test data for hyper-parameter tuning!!!\n",
        "3. Try to achieve a validation accuracy as high as possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hHpA1_nC3HV"
      },
      "source": [
        "### Remark: \n",
        "\n",
        "The following CNN is just an example. You are supposed to make **substantial improvements** such as:\n",
        "* Add more layers.\n",
        "* Use regularizations, e.g., dropout.\n",
        "* Use batch normalization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCAMiHTAC3HW"
      },
      "source": [
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "\n",
        "def initModel():\n",
        "  model = Sequential()\n",
        "  # The first two layers with 32 filters of window size 3x3\n",
        "  model.add(Conv2D(32, (3, 3), padding='same', input_shape=(32,32,3)))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.Activation('relu'))\n",
        "  model.add(Conv2D(32, (3, 3)))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.Activation('relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  \n",
        "  model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.Activation('relu'))\n",
        "  model.add(Conv2D(64, (3, 3)))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.Activation('relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "  model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.Activation('relu'))\n",
        "\n",
        "  model.add(Conv2D(64, (3, 3)))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.Activation('relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(512))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.Activation('relu'))\n",
        "  model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "  model.summary()\n",
        "  return model"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z04JLKLrC3HW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a300b279-0d8f-4772-986d-f99fa19bb7b0"
      },
      "source": [
        "from keras import optimizers\n",
        "\n",
        "learning_rate = 0.0001 # to be tuned! 1E-5\n",
        "model_40k=initModel()\n",
        "model_40k.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=learning_rate),\n",
        "              metrics=['acc'])\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_48 (Conv2D)           (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "batch_normalization_56 (Batc (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "activation_56 (Activation)   (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_49 (Conv2D)           (None, 30, 30, 32)        9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_57 (Batc (None, 30, 30, 32)        128       \n",
            "_________________________________________________________________\n",
            "activation_57 (Activation)   (None, 30, 30, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_24 (MaxPooling (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_50 (Conv2D)           (None, 15, 15, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_58 (Batc (None, 15, 15, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_58 (Activation)   (None, 15, 15, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_51 (Conv2D)           (None, 13, 13, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_59 (Batc (None, 13, 13, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_59 (Activation)   (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_25 (MaxPooling (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_52 (Conv2D)           (None, 6, 6, 64)          36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_60 (Batc (None, 6, 6, 64)          256       \n",
            "_________________________________________________________________\n",
            "activation_60 (Activation)   (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_53 (Conv2D)           (None, 4, 4, 64)          36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_61 (Batc (None, 4, 4, 64)          256       \n",
            "_________________________________________________________________\n",
            "activation_61 (Activation)   (None, 4, 4, 64)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_26 (MaxPooling (None, 2, 2, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_8 (Flatten)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 512)               131584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_62 (Batc (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "activation_62 (Activation)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 279,466\n",
            "Trainable params: 277,802\n",
            "Non-trainable params: 1,664\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvz2Sh3kC3HW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83de17b5-204f-42ef-8fd0-4d66ef975ace"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "rotation_range=40,\n",
        "zoom_range=0.2,\n",
        "horizontal_flip=True)\n",
        "\n",
        "bsize = 50\n",
        "\n",
        "train_gen = train_datagen.flow(x_tr, y_tr, batch_size = bsize)\n",
        "\n",
        "history = model_40k.fit_generator(train_gen, steps_per_epoch = len(x_tr)//bsize, epochs = 100, validation_data=(x_val, y_val))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "800/800 [==============================] - 35s 41ms/step - loss: 2.1368 - acc: 0.2326 - val_loss: 1.6816 - val_acc: 0.4008\n",
            "Epoch 2/100\n",
            "800/800 [==============================] - 32s 41ms/step - loss: 1.6872 - acc: 0.3831 - val_loss: 1.5985 - val_acc: 0.4366\n",
            "Epoch 3/100\n",
            "800/800 [==============================] - 32s 40ms/step - loss: 1.5647 - acc: 0.4304 - val_loss: 1.4910 - val_acc: 0.4732\n",
            "Epoch 4/100\n",
            "800/800 [==============================] - 32s 40ms/step - loss: 1.4722 - acc: 0.4673 - val_loss: 1.6308 - val_acc: 0.4516\n",
            "Epoch 5/100\n",
            "800/800 [==============================] - 32s 40ms/step - loss: 1.4224 - acc: 0.4864 - val_loss: 1.3895 - val_acc: 0.5144\n",
            "Epoch 6/100\n",
            "800/800 [==============================] - 32s 40ms/step - loss: 1.3591 - acc: 0.5099 - val_loss: 1.1883 - val_acc: 0.5717\n",
            "Epoch 7/100\n",
            "800/800 [==============================] - 32s 40ms/step - loss: 1.3220 - acc: 0.5263 - val_loss: 1.1566 - val_acc: 0.5891\n",
            "Epoch 8/100\n",
            "800/800 [==============================] - 32s 40ms/step - loss: 1.2909 - acc: 0.5394 - val_loss: 1.2227 - val_acc: 0.5638\n",
            "Epoch 9/100\n",
            "800/800 [==============================] - 32s 40ms/step - loss: 1.2513 - acc: 0.5511 - val_loss: 1.1820 - val_acc: 0.5782\n",
            "Epoch 10/100\n",
            "800/800 [==============================] - 32s 40ms/step - loss: 1.2114 - acc: 0.5640 - val_loss: 1.2851 - val_acc: 0.5602\n",
            "Epoch 11/100\n",
            "800/800 [==============================] - 32s 40ms/step - loss: 1.1926 - acc: 0.5764 - val_loss: 1.1079 - val_acc: 0.5938\n",
            "Epoch 12/100\n",
            "800/800 [==============================] - 32s 40ms/step - loss: 1.1558 - acc: 0.5877 - val_loss: 1.0265 - val_acc: 0.6344\n",
            "Epoch 13/100\n",
            "800/800 [==============================] - 32s 40ms/step - loss: 1.1482 - acc: 0.5932 - val_loss: 1.1361 - val_acc: 0.6117\n",
            "Epoch 14/100\n",
            "800/800 [==============================] - 32s 40ms/step - loss: 1.1166 - acc: 0.6034 - val_loss: 1.0051 - val_acc: 0.6452\n",
            "Epoch 15/100\n",
            "800/800 [==============================] - 31s 39ms/step - loss: 1.0966 - acc: 0.6099 - val_loss: 1.0043 - val_acc: 0.6516\n",
            "Epoch 16/100\n",
            "800/800 [==============================] - 32s 40ms/step - loss: 1.0847 - acc: 0.6115 - val_loss: 0.9082 - val_acc: 0.6750\n",
            "Epoch 17/100\n",
            "800/800 [==============================] - 32s 40ms/step - loss: 1.0611 - acc: 0.6244 - val_loss: 0.9676 - val_acc: 0.6621\n",
            "Epoch 18/100\n",
            "800/800 [==============================] - 32s 40ms/step - loss: 1.0545 - acc: 0.6314 - val_loss: 0.9398 - val_acc: 0.6642\n",
            "Epoch 19/100\n",
            "800/800 [==============================] - 32s 40ms/step - loss: 1.0172 - acc: 0.6374 - val_loss: 0.9900 - val_acc: 0.6513\n",
            "Epoch 20/100\n",
            "800/800 [==============================] - 32s 40ms/step - loss: 1.0125 - acc: 0.6425 - val_loss: 0.9996 - val_acc: 0.6522\n",
            "Epoch 21/100\n",
            "800/800 [==============================] - 31s 39ms/step - loss: 0.9962 - acc: 0.6465 - val_loss: 1.1406 - val_acc: 0.6301\n",
            "Epoch 22/100\n",
            "800/800 [==============================] - 32s 40ms/step - loss: 0.9814 - acc: 0.6529 - val_loss: 0.9058 - val_acc: 0.6809\n",
            "Epoch 23/100\n",
            "800/800 [==============================] - 32s 40ms/step - loss: 0.9649 - acc: 0.6623 - val_loss: 0.9435 - val_acc: 0.6671\n",
            "Epoch 24/100\n",
            "800/800 [==============================] - 31s 39ms/step - loss: 0.9570 - acc: 0.6650 - val_loss: 0.8287 - val_acc: 0.7060\n",
            "Epoch 25/100\n",
            "800/800 [==============================] - 31s 39ms/step - loss: 0.9599 - acc: 0.6612 - val_loss: 0.8086 - val_acc: 0.7137\n",
            "Epoch 26/100\n",
            "800/800 [==============================] - 32s 40ms/step - loss: 0.9297 - acc: 0.6737 - val_loss: 0.8056 - val_acc: 0.7158\n",
            "Epoch 27/100\n",
            "800/800 [==============================] - 31s 39ms/step - loss: 0.9273 - acc: 0.6724 - val_loss: 0.8845 - val_acc: 0.6925\n",
            "Epoch 28/100\n",
            "800/800 [==============================] - 31s 39ms/step - loss: 0.9119 - acc: 0.6820 - val_loss: 0.7925 - val_acc: 0.7195\n",
            "Epoch 29/100\n",
            "800/800 [==============================] - 32s 40ms/step - loss: 0.9012 - acc: 0.6829 - val_loss: 0.8198 - val_acc: 0.7132\n",
            "Epoch 30/100\n",
            "800/800 [==============================] - 31s 39ms/step - loss: 0.9057 - acc: 0.6838 - val_loss: 0.8351 - val_acc: 0.7083\n",
            "Epoch 31/100\n",
            "800/800 [==============================] - 31s 39ms/step - loss: 0.8938 - acc: 0.6853 - val_loss: 0.7601 - val_acc: 0.7300\n",
            "Epoch 32/100\n",
            "800/800 [==============================] - 32s 40ms/step - loss: 0.8767 - acc: 0.6949 - val_loss: 0.7558 - val_acc: 0.7325\n",
            "Epoch 33/100\n",
            "800/800 [==============================] - 31s 39ms/step - loss: 0.8847 - acc: 0.6948 - val_loss: 0.9642 - val_acc: 0.6767\n",
            "Epoch 34/100\n",
            "800/800 [==============================] - 31s 39ms/step - loss: 0.8647 - acc: 0.6966 - val_loss: 0.9708 - val_acc: 0.6823\n",
            "Epoch 35/100\n",
            "800/800 [==============================] - 32s 40ms/step - loss: 0.8628 - acc: 0.6990 - val_loss: 0.7972 - val_acc: 0.7239\n",
            "Epoch 36/100\n",
            "800/800 [==============================] - 32s 39ms/step - loss: 0.8526 - acc: 0.7030 - val_loss: 0.7698 - val_acc: 0.7334\n",
            "Epoch 37/100\n",
            "800/800 [==============================] - 31s 39ms/step - loss: 0.8512 - acc: 0.7060 - val_loss: 0.7608 - val_acc: 0.7360\n",
            "Epoch 38/100\n",
            "800/800 [==============================] - 32s 40ms/step - loss: 0.8367 - acc: 0.7071 - val_loss: 0.7843 - val_acc: 0.7366\n",
            "Epoch 39/100\n",
            "800/800 [==============================] - 31s 39ms/step - loss: 0.8368 - acc: 0.7103 - val_loss: 0.7592 - val_acc: 0.7365\n",
            "Epoch 40/100\n",
            "800/800 [==============================] - 31s 39ms/step - loss: 0.8269 - acc: 0.7119 - val_loss: 0.7173 - val_acc: 0.7453\n",
            "Epoch 41/100\n",
            "800/800 [==============================] - 32s 39ms/step - loss: 0.8299 - acc: 0.7122 - val_loss: 0.7023 - val_acc: 0.7563\n",
            "Epoch 42/100\n",
            "800/800 [==============================] - 31s 39ms/step - loss: 0.8146 - acc: 0.7166 - val_loss: 0.7255 - val_acc: 0.7478\n",
            "Epoch 43/100\n",
            "800/800 [==============================] - 31s 39ms/step - loss: 0.8035 - acc: 0.7231 - val_loss: 0.8042 - val_acc: 0.7298\n",
            "Epoch 44/100\n",
            "800/800 [==============================] - 32s 40ms/step - loss: 0.8020 - acc: 0.7195 - val_loss: 0.6722 - val_acc: 0.7681\n",
            "Epoch 45/100\n",
            "800/800 [==============================] - 31s 39ms/step - loss: 0.8051 - acc: 0.7236 - val_loss: 0.7841 - val_acc: 0.7328\n",
            "Epoch 46/100\n",
            "800/800 [==============================] - 31s 39ms/step - loss: 0.7900 - acc: 0.7229 - val_loss: 0.6704 - val_acc: 0.7640\n",
            "Epoch 47/100\n",
            "800/800 [==============================] - 31s 39ms/step - loss: 0.7742 - acc: 0.7285 - val_loss: 0.8536 - val_acc: 0.7152\n",
            "Epoch 48/100\n",
            "800/800 [==============================] - 32s 39ms/step - loss: 0.7808 - acc: 0.7279 - val_loss: 0.7047 - val_acc: 0.7596\n",
            "Epoch 49/100\n",
            "800/800 [==============================] - 31s 39ms/step - loss: 0.7747 - acc: 0.7292 - val_loss: 0.7545 - val_acc: 0.7458\n",
            "Epoch 50/100\n",
            "800/800 [==============================] - 31s 38ms/step - loss: 0.7749 - acc: 0.7313 - val_loss: 0.7036 - val_acc: 0.7548\n",
            "Epoch 51/100\n",
            "800/800 [==============================] - 31s 39ms/step - loss: 0.7724 - acc: 0.7297 - val_loss: 0.6954 - val_acc: 0.7609\n",
            "Epoch 52/100\n",
            "800/800 [==============================] - 31s 38ms/step - loss: 0.7579 - acc: 0.7355 - val_loss: 0.7476 - val_acc: 0.7521\n",
            "Epoch 53/100\n",
            "800/800 [==============================] - 31s 39ms/step - loss: 0.7559 - acc: 0.7385 - val_loss: 0.7364 - val_acc: 0.7465\n",
            "Epoch 54/100\n",
            "800/800 [==============================] - 31s 39ms/step - loss: 0.7717 - acc: 0.7313 - val_loss: 0.6177 - val_acc: 0.7885\n",
            "Epoch 55/100\n",
            "800/800 [==============================] - 31s 39ms/step - loss: 0.7536 - acc: 0.7387 - val_loss: 0.7528 - val_acc: 0.7455\n",
            "Epoch 56/100\n",
            "800/800 [==============================] - 31s 38ms/step - loss: 0.7374 - acc: 0.7433 - val_loss: 0.7550 - val_acc: 0.7464\n",
            "Epoch 57/100\n",
            "800/800 [==============================] - 31s 39ms/step - loss: 0.7397 - acc: 0.7452 - val_loss: 0.7134 - val_acc: 0.7584\n",
            "Epoch 58/100\n",
            "800/800 [==============================] - 31s 39ms/step - loss: 0.7402 - acc: 0.7427 - val_loss: 0.6829 - val_acc: 0.7693\n",
            "Epoch 59/100\n",
            "800/800 [==============================] - 31s 38ms/step - loss: 0.7329 - acc: 0.7467 - val_loss: 0.7852 - val_acc: 0.7413\n",
            "Epoch 60/100\n",
            "800/800 [==============================] - 31s 39ms/step - loss: 0.7400 - acc: 0.7412 - val_loss: 0.7252 - val_acc: 0.7551\n",
            "Epoch 61/100\n",
            "800/800 [==============================] - 31s 39ms/step - loss: 0.7197 - acc: 0.7489 - val_loss: 0.7345 - val_acc: 0.7547\n",
            "Epoch 62/100\n",
            "800/800 [==============================] - 31s 38ms/step - loss: 0.7302 - acc: 0.7469 - val_loss: 0.6686 - val_acc: 0.7717\n",
            "Epoch 63/100\n",
            "800/800 [==============================] - 31s 39ms/step - loss: 0.7270 - acc: 0.7479 - val_loss: 0.6749 - val_acc: 0.7706\n",
            "Epoch 64/100\n",
            "800/800 [==============================] - 31s 39ms/step - loss: 0.7329 - acc: 0.7466 - val_loss: 0.6741 - val_acc: 0.7697\n",
            "Epoch 65/100\n",
            "800/800 [==============================] - 31s 39ms/step - loss: 0.7220 - acc: 0.7493 - val_loss: 0.6438 - val_acc: 0.7780\n",
            "Epoch 66/100\n",
            "800/800 [==============================] - 31s 39ms/step - loss: 0.7187 - acc: 0.7515 - val_loss: 0.6575 - val_acc: 0.7764\n",
            "Epoch 67/100\n",
            "800/800 [==============================] - 31s 39ms/step - loss: 0.7089 - acc: 0.7510 - val_loss: 0.6594 - val_acc: 0.7749\n",
            "Epoch 68/100\n",
            "800/800 [==============================] - 31s 38ms/step - loss: 0.7038 - acc: 0.7583 - val_loss: 0.6079 - val_acc: 0.7955\n",
            "Epoch 69/100\n",
            "800/800 [==============================] - 31s 39ms/step - loss: 0.7065 - acc: 0.7564 - val_loss: 0.6435 - val_acc: 0.7832\n",
            "Epoch 70/100\n",
            "800/800 [==============================] - 31s 38ms/step - loss: 0.7020 - acc: 0.7578 - val_loss: 0.6376 - val_acc: 0.7832\n",
            "Epoch 71/100\n",
            "800/800 [==============================] - 30s 38ms/step - loss: 0.7128 - acc: 0.7556 - val_loss: 0.6983 - val_acc: 0.7662\n",
            "Epoch 72/100\n",
            "800/800 [==============================] - 31s 39ms/step - loss: 0.6896 - acc: 0.7596 - val_loss: 0.6357 - val_acc: 0.7856\n",
            "Epoch 73/100\n",
            "800/800 [==============================] - 31s 38ms/step - loss: 0.6950 - acc: 0.7577 - val_loss: 0.7408 - val_acc: 0.7543\n",
            "Epoch 74/100\n",
            "800/800 [==============================] - 30s 38ms/step - loss: 0.7045 - acc: 0.7568 - val_loss: 0.6547 - val_acc: 0.7817\n",
            "Epoch 75/100\n",
            "800/800 [==============================] - 31s 39ms/step - loss: 0.6807 - acc: 0.7633 - val_loss: 0.6414 - val_acc: 0.7811\n",
            "Epoch 76/100\n",
            "800/800 [==============================] - 31s 39ms/step - loss: 0.6768 - acc: 0.7646 - val_loss: 0.7188 - val_acc: 0.7630\n",
            "Epoch 77/100\n",
            "800/800 [==============================] - 30s 38ms/step - loss: 0.6860 - acc: 0.7605 - val_loss: 0.6041 - val_acc: 0.7995\n",
            "Epoch 78/100\n",
            "800/800 [==============================] - 32s 40ms/step - loss: 0.6873 - acc: 0.7620 - val_loss: 0.6165 - val_acc: 0.7895\n",
            "Epoch 79/100\n",
            "800/800 [==============================] - 31s 39ms/step - loss: 0.6656 - acc: 0.7688 - val_loss: 0.6395 - val_acc: 0.7847\n",
            "Epoch 80/100\n",
            "800/800 [==============================] - 30s 38ms/step - loss: 0.6747 - acc: 0.7664 - val_loss: 0.6091 - val_acc: 0.7937\n",
            "Epoch 81/100\n",
            "800/800 [==============================] - 31s 39ms/step - loss: 0.6823 - acc: 0.7657 - val_loss: 0.6334 - val_acc: 0.7905\n",
            "Epoch 82/100\n",
            "800/800 [==============================] - 31s 38ms/step - loss: 0.6710 - acc: 0.7676 - val_loss: 0.6139 - val_acc: 0.7894\n",
            "Epoch 83/100\n",
            "800/800 [==============================] - 30s 38ms/step - loss: 0.6656 - acc: 0.7706 - val_loss: 0.6241 - val_acc: 0.7870\n",
            "Epoch 84/100\n",
            "800/800 [==============================] - 32s 40ms/step - loss: 0.6624 - acc: 0.7720 - val_loss: 0.6340 - val_acc: 0.7854\n",
            "Epoch 85/100\n",
            "800/800 [==============================] - 31s 38ms/step - loss: 0.6637 - acc: 0.7677 - val_loss: 0.5959 - val_acc: 0.7978\n",
            "Epoch 86/100\n",
            "800/800 [==============================] - 30s 38ms/step - loss: 0.6652 - acc: 0.7699 - val_loss: 0.6542 - val_acc: 0.7838\n",
            "Epoch 87/100\n",
            "800/800 [==============================] - 31s 39ms/step - loss: 0.6603 - acc: 0.7677 - val_loss: 0.6331 - val_acc: 0.7891\n",
            "Epoch 88/100\n",
            "800/800 [==============================] - 31s 38ms/step - loss: 0.6555 - acc: 0.7706 - val_loss: 0.6072 - val_acc: 0.7907\n",
            "Epoch 89/100\n",
            "800/800 [==============================] - 30s 38ms/step - loss: 0.6611 - acc: 0.7686 - val_loss: 0.6912 - val_acc: 0.7674\n",
            "Epoch 90/100\n",
            "800/800 [==============================] - 31s 39ms/step - loss: 0.6633 - acc: 0.7698 - val_loss: 0.5922 - val_acc: 0.8027\n",
            "Epoch 91/100\n",
            "800/800 [==============================] - 31s 38ms/step - loss: 0.6373 - acc: 0.7836 - val_loss: 0.6176 - val_acc: 0.7943\n",
            "Epoch 92/100\n",
            "800/800 [==============================] - 31s 38ms/step - loss: 0.6457 - acc: 0.7820 - val_loss: 0.5887 - val_acc: 0.8000\n",
            "Epoch 93/100\n",
            "800/800 [==============================] - 31s 39ms/step - loss: 0.6430 - acc: 0.7777 - val_loss: 0.6086 - val_acc: 0.7964\n",
            "Epoch 94/100\n",
            "800/800 [==============================] - 31s 39ms/step - loss: 0.6558 - acc: 0.7722 - val_loss: 0.5694 - val_acc: 0.8033\n",
            "Epoch 95/100\n",
            "800/800 [==============================] - 30s 38ms/step - loss: 0.6390 - acc: 0.7798 - val_loss: 0.5825 - val_acc: 0.8026\n",
            "Epoch 96/100\n",
            "800/800 [==============================] - 31s 39ms/step - loss: 0.6294 - acc: 0.7818 - val_loss: 0.6178 - val_acc: 0.7914\n",
            "Epoch 97/100\n",
            "800/800 [==============================] - 31s 39ms/step - loss: 0.6406 - acc: 0.7810 - val_loss: 0.6300 - val_acc: 0.7843\n",
            "Epoch 98/100\n",
            "800/800 [==============================] - 31s 38ms/step - loss: 0.6332 - acc: 0.7813 - val_loss: 0.6599 - val_acc: 0.7831\n",
            "Epoch 99/100\n",
            "800/800 [==============================] - 31s 39ms/step - loss: 0.6334 - acc: 0.7789 - val_loss: 0.6283 - val_acc: 0.7915\n",
            "Epoch 100/100\n",
            "800/800 [==============================] - 31s 39ms/step - loss: 0.6388 - acc: 0.7790 - val_loss: 0.5934 - val_acc: 0.8004\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zISafKdFC3HW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "dde22e53-5242-47a9-865f-bf343cbadd6e"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7T6K7SG4C3HX"
      },
      "source": [
        "## 3. Train (again) and evaluate the model\n",
        "\n",
        "- To this end, you have found the \"best\" hyper-parameters. \n",
        "- Now, fix the hyper-parameters and train the network on the entire training set (all the 50K training samples)\n",
        "- Evaluate your model on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzvQxO4NC3HX"
      },
      "source": [
        "### 3.1. Train the model on the entire training set\n",
        "\n",
        "Why? Previously, you used 40K samples for training; you wasted 10K samples for the sake of hyper-parameter tuning. Now you already know the hyper-parameters, so why not using all the 50K samples for training?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYfsePQnC3HY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a22dcf4-e1f2-45c1-9c29-6a451738ae7d"
      },
      "source": [
        "#<Compile your model again (using the same hyper-parameters)>\n",
        "#...\n",
        "\n",
        "learning_rate = 0.001\n",
        "model_50k=initModel()\n",
        "model_50k.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=learning_rate),\n",
        "              metrics=['acc'])"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_42 (Conv2D)           (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "batch_normalization_49 (Batc (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "activation_49 (Activation)   (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_43 (Conv2D)           (None, 30, 30, 32)        9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_50 (Batc (None, 30, 30, 32)        128       \n",
            "_________________________________________________________________\n",
            "activation_50 (Activation)   (None, 30, 30, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_21 (MaxPooling (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_44 (Conv2D)           (None, 15, 15, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_51 (Batc (None, 15, 15, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_51 (Activation)   (None, 15, 15, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_45 (Conv2D)           (None, 13, 13, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_52 (Batc (None, 13, 13, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_52 (Activation)   (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_22 (MaxPooling (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_46 (Conv2D)           (None, 6, 6, 64)          36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_53 (Batc (None, 6, 6, 64)          256       \n",
            "_________________________________________________________________\n",
            "activation_53 (Activation)   (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_47 (Conv2D)           (None, 4, 4, 64)          36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_54 (Batc (None, 4, 4, 64)          256       \n",
            "_________________________________________________________________\n",
            "activation_54 (Activation)   (None, 4, 4, 64)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_23 (MaxPooling (None, 2, 2, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 512)               131584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_55 (Batc (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "activation_55 (Activation)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 279,466\n",
            "Trainable params: 277,802\n",
            "Non-trainable params: 1,664\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oV3yoLtlC3HY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5130f9f-1b75-4e71-ceb7-1b6adc4ed818"
      },
      "source": [
        "#<Train your model on the entire training set (50K samples)>\n",
        "#<Use (x_train, y_train_vec) instead of (x_tr, y_tr)>\n",
        "#<Do NOT use the validation_data option (because now you do not have validation data)>\n",
        "#...\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "rotation_range=40,\n",
        "zoom_range=0.2,\n",
        "horizontal_flip=True)\n",
        "\n",
        "bsize = 50\n",
        "\n",
        "train_gen = train_datagen.flow(x_train, y_train_vec, batch_size = bsize)\n",
        "\n",
        "total_train_history = model_50k.fit_generator(train_gen, steps_per_epoch = len(x_train)//bsize, epochs = 100)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1000/1000 [==============================] - 41s 39ms/step - loss: 1.8308 - acc: 0.3433\n",
            "Epoch 2/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 1.3248 - acc: 0.5234\n",
            "Epoch 3/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 1.1459 - acc: 0.5927\n",
            "Epoch 4/100\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 1.0666 - acc: 0.6246\n",
            "Epoch 5/100\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.9996 - acc: 0.6518\n",
            "Epoch 6/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.9507 - acc: 0.6741\n",
            "Epoch 7/100\n",
            "1000/1000 [==============================] - 40s 40ms/step - loss: 0.9102 - acc: 0.6854\n",
            "Epoch 8/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.8831 - acc: 0.7019\n",
            "Epoch 9/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.8529 - acc: 0.7055\n",
            "Epoch 10/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.8223 - acc: 0.7203\n",
            "Epoch 11/100\n",
            "1000/1000 [==============================] - 40s 40ms/step - loss: 0.8094 - acc: 0.7210\n",
            "Epoch 12/100\n",
            "1000/1000 [==============================] - 40s 40ms/step - loss: 0.7971 - acc: 0.7282\n",
            "Epoch 13/100\n",
            "1000/1000 [==============================] - 40s 40ms/step - loss: 0.7618 - acc: 0.7407\n",
            "Epoch 14/100\n",
            "1000/1000 [==============================] - 40s 40ms/step - loss: 0.7601 - acc: 0.7426\n",
            "Epoch 15/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.7461 - acc: 0.7465\n",
            "Epoch 16/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.7223 - acc: 0.7529\n",
            "Epoch 17/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.7087 - acc: 0.7561\n",
            "Epoch 18/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.7033 - acc: 0.7606\n",
            "Epoch 19/100\n",
            "1000/1000 [==============================] - 40s 39ms/step - loss: 0.7036 - acc: 0.7610\n",
            "Epoch 20/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.6854 - acc: 0.7691\n",
            "Epoch 21/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.6927 - acc: 0.7647\n",
            "Epoch 22/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.6776 - acc: 0.7731\n",
            "Epoch 23/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.6545 - acc: 0.7803\n",
            "Epoch 24/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.6577 - acc: 0.7776\n",
            "Epoch 25/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.6509 - acc: 0.7766\n",
            "Epoch 26/100\n",
            "1000/1000 [==============================] - 40s 39ms/step - loss: 0.6390 - acc: 0.7842\n",
            "Epoch 27/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.6436 - acc: 0.7796\n",
            "Epoch 28/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.6344 - acc: 0.7853\n",
            "Epoch 29/100\n",
            "1000/1000 [==============================] - 40s 40ms/step - loss: 0.6292 - acc: 0.7868\n",
            "Epoch 30/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.6295 - acc: 0.7872\n",
            "Epoch 31/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.6177 - acc: 0.7878\n",
            "Epoch 32/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.6103 - acc: 0.7921\n",
            "Epoch 33/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.6116 - acc: 0.7910\n",
            "Epoch 34/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.6091 - acc: 0.7940\n",
            "Epoch 35/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.5990 - acc: 0.7961\n",
            "Epoch 36/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.5943 - acc: 0.7996\n",
            "Epoch 37/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.6057 - acc: 0.7960\n",
            "Epoch 38/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.5856 - acc: 0.8002\n",
            "Epoch 39/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.5894 - acc: 0.8042\n",
            "Epoch 40/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.5823 - acc: 0.8037\n",
            "Epoch 41/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.5862 - acc: 0.7995\n",
            "Epoch 42/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.5838 - acc: 0.8022\n",
            "Epoch 43/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.5708 - acc: 0.8080\n",
            "Epoch 44/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.5794 - acc: 0.8046\n",
            "Epoch 45/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.5759 - acc: 0.8054\n",
            "Epoch 46/100\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.5594 - acc: 0.8098\n",
            "Epoch 47/100\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.5629 - acc: 0.8065\n",
            "Epoch 48/100\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.5507 - acc: 0.8120\n",
            "Epoch 49/100\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.5535 - acc: 0.8118\n",
            "Epoch 50/100\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.5534 - acc: 0.8148\n",
            "Epoch 51/100\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.5409 - acc: 0.8194\n",
            "Epoch 52/100\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.5551 - acc: 0.8108\n",
            "Epoch 53/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.5530 - acc: 0.8141\n",
            "Epoch 54/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.5500 - acc: 0.8132\n",
            "Epoch 55/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.5566 - acc: 0.8114\n",
            "Epoch 56/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.5438 - acc: 0.8160\n",
            "Epoch 57/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.5482 - acc: 0.8145\n",
            "Epoch 58/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.5492 - acc: 0.8151\n",
            "Epoch 59/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.5453 - acc: 0.8162\n",
            "Epoch 60/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.5357 - acc: 0.8163\n",
            "Epoch 61/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.5395 - acc: 0.8152\n",
            "Epoch 62/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.5330 - acc: 0.8200\n",
            "Epoch 63/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.5318 - acc: 0.8213\n",
            "Epoch 64/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.5404 - acc: 0.8167\n",
            "Epoch 65/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.5328 - acc: 0.8201\n",
            "Epoch 66/100\n",
            "1000/1000 [==============================] - 40s 39ms/step - loss: 0.5324 - acc: 0.8232\n",
            "Epoch 67/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.5231 - acc: 0.8242\n",
            "Epoch 68/100\n",
            "1000/1000 [==============================] - 40s 40ms/step - loss: 0.5217 - acc: 0.8238\n",
            "Epoch 69/100\n",
            "1000/1000 [==============================] - 40s 40ms/step - loss: 0.5312 - acc: 0.8200\n",
            "Epoch 70/100\n",
            "1000/1000 [==============================] - 40s 40ms/step - loss: 0.5164 - acc: 0.8242\n",
            "Epoch 71/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.5265 - acc: 0.8221\n",
            "Epoch 72/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.5149 - acc: 0.8286\n",
            "Epoch 73/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.5213 - acc: 0.8254\n",
            "Epoch 74/100\n",
            "1000/1000 [==============================] - 40s 40ms/step - loss: 0.5161 - acc: 0.8235\n",
            "Epoch 75/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.5235 - acc: 0.8222\n",
            "Epoch 76/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.5157 - acc: 0.8251\n",
            "Epoch 77/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.5211 - acc: 0.8222\n",
            "Epoch 78/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.5038 - acc: 0.8280\n",
            "Epoch 79/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.5193 - acc: 0.8241\n",
            "Epoch 80/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.5116 - acc: 0.8257\n",
            "Epoch 81/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.5107 - acc: 0.8266\n",
            "Epoch 82/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.5148 - acc: 0.8289\n",
            "Epoch 83/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.5109 - acc: 0.8272\n",
            "Epoch 84/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.5055 - acc: 0.8265\n",
            "Epoch 85/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.5139 - acc: 0.8270\n",
            "Epoch 86/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.5114 - acc: 0.8277\n",
            "Epoch 87/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.5121 - acc: 0.8275\n",
            "Epoch 88/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.5047 - acc: 0.8313\n",
            "Epoch 89/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.4918 - acc: 0.8331\n",
            "Epoch 90/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.5049 - acc: 0.8293\n",
            "Epoch 91/100\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 0.4966 - acc: 0.8328\n",
            "Epoch 92/100\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.5018 - acc: 0.8317\n",
            "Epoch 93/100\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.4945 - acc: 0.8311\n",
            "Epoch 94/100\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.4895 - acc: 0.8332\n",
            "Epoch 95/100\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.4924 - acc: 0.8328\n",
            "Epoch 96/100\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.4867 - acc: 0.8355\n",
            "Epoch 97/100\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.4955 - acc: 0.8333\n",
            "Epoch 98/100\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.4926 - acc: 0.8348\n",
            "Epoch 99/100\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.4871 - acc: 0.8375\n",
            "Epoch 100/100\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.5036 - acc: 0.8304\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIB6I68iC3HY"
      },
      "source": [
        "### 3.2. Evaluate the model on the test set\n",
        "\n",
        "Do NOT used the test set until now. Make sure that your model parameters and hyper-parameters are independent of the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Yr_HdXIC3HY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e92dd560-bb7e-46cc-cc0d-0d33ce5b205b"
      },
      "source": [
        "loss_and_acc = model_50k.evaluate(x_test, y_test_vec)\n",
        "print('loss = ' + str(loss_and_acc[0]))\n",
        "print('accuracy = ' + str(loss_and_acc[1]))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 2s 6ms/step - loss: 0.4700 - acc: 0.8467\n",
            "loss = 0.4699772000312805\n",
            "accuracy = 0.8467000126838684\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtV00NFuCwVi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}